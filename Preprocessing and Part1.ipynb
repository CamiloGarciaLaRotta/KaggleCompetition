{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d42190e34d49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import random\n",
    "from operator import itemgetter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "from itertools import count\n",
    "iid = count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_X = np.loadtxt(\"./Datasets/mini_train_x.csv\", delimiter=\",\")\n",
    "mini_y = np.loadtxt(\"./Datasets/mini_train_y.csv\", delimiter=\",\")\n",
    "\n",
    "# mini_X = np.loadtxt(\"./Datasets/train_x.csv\", delimiter=\",\")\n",
    "# mini_y = np.loadtxt(\"./Datasets/train_y.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape to easily visualize while cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = mini_X.reshape(-1, 64, 64)\n",
    "\n",
    "imgs = []\n",
    "for idx in range(len(x)):\n",
    "    img_id = next(iid)\n",
    "    cv2.imwrite('Output/{}.jpg'.format(img_id), 255-x[idx])\n",
    "    imgs.append(cv2.imread('Output/{}.jpg'.format(img_id), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first transform to binary image (black, white):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bin_imgs = []\n",
    "for img in imgs:\n",
    "    _, bin_img = cv2.threshold(img, 50, 255, cv2.THRESH_BINARY)\n",
    "    bin_imgs.append(bin_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will remove the noice by dilatating and eroding the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised_imgs = []\n",
    "for bin_img in bin_imgs:\n",
    "    dilatated = cv2.dilate(bin_img, np.ones((3,3)))\n",
    "    denoised_imgs.append(cv2.erode(dilatated, np.ones((2,2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, titles):\n",
    "    cols = len(images)\n",
    "    rows = len(images[0])\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            plt.subplot(1,cols,c+1)\n",
    "            plt.imshow(images[c][r],'gray')\n",
    "            plt.title(titles[c])\n",
    "            plt.xticks([]),plt.yticks([])\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "titles = ['Original Image', 'Binary', 'Dilatated & Eroded']\n",
    "images = [imgs[:5], bin_imgs[:5], denoised_imgs[:5]]\n",
    "plot_images(images, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will blur in order to smooth any edges __(TEST IF THIS IS ACTUALLY USEFUL)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurred_imgs = []\n",
    "for denoised_img in denoised_imgs:\n",
    "    blurred_imgs.append(cv2.blur(denoised_img,(2,2)))\n",
    "\n",
    "random_idx = 4\n",
    "blur = cv2.blur(denoised_imgs[random_idx],(2,2))\n",
    "gaussian_blur = cv2.GaussianBlur(denoised_imgs[random_idx],(3,3),0)    \n",
    "plt.subplot(151),plt.imshow(imgs[random_idx],'gray'),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(152),plt.imshow(bin_imgs[random_idx],'gray'),plt.title('Binary')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(153),plt.imshow(denoised_imgs[random_idx],'gray'),plt.title('Denoised')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(154),plt.imshow(gaussian_blur,'gray'),plt.title('Gaussian Blur')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(155),plt.imshow(blur,'gray'),plt.title('Pixel Blur')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter permutations\n",
    "SEED = 42    # random state seed for consistent results\n",
    "FOLDS = 3    # number of folds for K-fold cross validation\n",
    "DUAL = [False, True] # WHEN n_samples > n_features DUAL=FALSE\n",
    "LOSSES = ['squared_hinge','hinge']\n",
    "PENALTIES = ['l1','l2']\n",
    "SOLVERS = ['lbfgs'] # 'newton-cg','sag','saga',\n",
    "# error_coefs = np.logspace(-0.01, 2, 15)\n",
    "# FOUND OUT A SMALLER RANGE OF GOOD VALUES\n",
    "error_coefs = np.linspace(20,80, 10)\n",
    "\n",
    "KFOLD = KFold(n_splits=FOLDS, random_state=SEED)\n",
    "\n",
    "clean_X = [img.flatten() for img in denoised_imgs]\n",
    "mini_X_train, mini_X_test, mini_y_train, mini_y_test = train_test_split(clean_X, mini_y, \n",
    "                                                                        test_size=0.3,\n",
    "                                                                        random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper methods\n",
    "Methods to generate all possible permutations of HyperParameters \n",
    "for LinearSVMs and Logistic Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_hp_permutations():\n",
    "    clfs = []\n",
    "    for d in DUAL:\n",
    "        for l in LOSSES:\n",
    "            for p in PENALTIES:\n",
    "                # ignore unvalid combinations\n",
    "                if not d and p == 'l2': continue\n",
    "                if p == 'l1' and l == 'hinge': continue\n",
    "                if d and p == 'l1' and l == 'squared_hinge': continue\n",
    "\n",
    "                for c in error_coefs:\n",
    "                    clfs.append(LinearSVC(C=c,dual=d,loss=l,penalty=p))\n",
    "    \n",
    "    return clfs\n",
    "\n",
    "def get_logreg_hp_permutations():\n",
    "    clfs = []\n",
    "#     for d in DUAL:\n",
    "    for p in PENALTIES:\n",
    "        for s in SOLVERS:\n",
    "            # ignore unvalid combinations\n",
    "#             if not d and p == 'l2': continue\n",
    "            if s != 'saga' and p == 'l1': continue\n",
    "\n",
    "            for c in error_coefs:\n",
    "                clfs.append(LogisticRegression(C=c,penalty=p,solver=s,n_jobs=8))\n",
    "    \n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base and dummy clf performances for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy clf performance\n",
    "random_clf = DummyClassifier(random_state=42)\n",
    "result = cross_val_score(random_clf, mini_X_train, mini_y_train, cv=KFOLD, scoring='f1_micro')\n",
    "print('Random clf performance: {:.4f}'.format(result.mean()))\n",
    "\n",
    "# Base performance (default HP)\n",
    "result = cross_val_score(LinearSVC(), mini_X_train, mini_y_train, cv=KFOLD, scoring='f1_micro')\n",
    "print('LinearSVM base performance: {:.4f}'.format(result.mean()))\n",
    "\n",
    "result = cross_val_score(LogisticRegression(), mini_X_train, mini_y_train, cv=KFOLD, scoring='f1_micro')\n",
    "print('LogReg base performance: {:.4f}'.format(result.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Model Selection for LinearSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "# create preprocessing pipeline\n",
    "for specific_svm_permutation in get_svm_hp_permutations():\n",
    "    estimators = []\n",
    "    estimators.append(('standardize', StandardScaler()))\n",
    "    estimators.append(('clf', specific_svm_permutation))\n",
    "    model = Pipeline(estimators)\n",
    "    result = cross_val_score(model, mini_X_train, mini_y_train, cv=KFOLD, scoring='f1_micro')\n",
    "\n",
    "    scores.append((specific_svm_permutation, result.mean()))\n",
    "    \n",
    "best_svm, max_train = max(scores,key=itemgetter(1)) \n",
    "print('Best SVM: \\n{}'.format(best_svm))\n",
    "print('TRAIN SCORE: {:.4f}'.format(max_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Model Selection for LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "# create preprocessing pipeline\n",
    "for specific_logreg_permutation in get_logreg_hp_permutations():\n",
    "    estimators = []\n",
    "    estimators.append(('standardize', StandardScaler()))\n",
    "    estimators.append(('clf', specific_logreg_permutation))\n",
    "    model = Pipeline(estimators)\n",
    "    result = cross_val_score(model, mini_X_train, mini_y_train, cv=KFOLD, scoring='f1_micro')\n",
    "\n",
    "    scores.append((specific_logreg_permutation, result.mean()))\n",
    "    \n",
    "best_svm, max_train = max(scores,key=itemgetter(1)) \n",
    "\n",
    "print('Best SVM: \\n{}'.format(best_svm))\n",
    "print('TRAIN SCORE: {:.4f}'.format(max_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
