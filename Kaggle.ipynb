{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shuffle two arrays, keeping rows in correspondence\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def linear_svm(train, valid, c_vals, verbose=False):\n",
    "    train_x = train[0]\n",
    "    train_y = train[1]\n",
    "    valid_x = valid[0]\n",
    "    valid_y = valid[1]\n",
    "\n",
    "    best_classifier = (None, 0, \"l1\", 0)\n",
    "\n",
    "    res = {\"l1\": [], \"l2\": []}\n",
    "    \n",
    "    for penalty in [\"l2\"]:\n",
    "        for c in c_vals:\n",
    "            clf = LinearSVC(C=c, penalty=penalty)\n",
    "            clf.fit(train_x, train_y)\n",
    "            prediction = clf.predict(valid_x)\n",
    "            accuracy = accuracy_score(valid_y, prediction)\n",
    "            res[penalty].append(accuracy)\n",
    "            if (accuracy > best_classifier[3]):\n",
    "                best_classifier = (clf, c, penalty, accuracy)\n",
    "            if verbose:\n",
    "                print(\"Tried c = \" + str(c) + \" with \" + penalty + \" penalty\")\n",
    "    #plt.plot(c_vals, res[\"l1\"])\n",
    "    plt.plot(c_vals, res[\"l2\"])\n",
    "    #plt.legend([\"l1 penalty\", \"l2 penalty\"])\n",
    "    plt.xlabel(\"C parameter\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xscale(\"log\", basex=10)\n",
    "    plt.title(\"Results from linear SVM classifier\")\n",
    "    plt.show()\n",
    "    return (best_classifier[0], {\"penalty\": best_classifier[2], \"c\": best_classifier[1]}, best_classifier[3])\n",
    "\n",
    "# The training and test sets are passed as tuples where the first index is the X and the second is the Y\n",
    "def logistic_regression(train, valid, c_vals, verbose=False):\n",
    "    train_x = train[0]\n",
    "    train_y = train[1]\n",
    "    valid_x = valid[0]\n",
    "    valid_y = valid[1]\n",
    "    \n",
    "    best_classifier = (None, 0, \"l1\", 0)\n",
    "\n",
    "    res = {\"l1\": [], \"l2\": []}\n",
    "    \n",
    "    for penalty in [\"l1\", \"l2\"]:\n",
    "        for c in c_vals:\n",
    "            clf = LogisticRegression(C=c, penalty=penalty)\n",
    "            clf.fit(train_x, train_y)\n",
    "            prediction = clf.predict(valid_x)\n",
    "            accuracy = accuracy_score(valid_y, prediction)\n",
    "            res[penalty].append(accuracy)\n",
    "            if (accuracy > best_classifier[3]):\n",
    "                best_classifier = (clf, c, penalty, accuracy)\n",
    "            if verbose:\n",
    "                print(\"Tried c = \" + str(c) + \" with \" + penalty + \" penalty\")\n",
    "    plt.plot(c_vals, res[\"l1\"])\n",
    "    plt.plot(c_vals, res[\"l2\"])\n",
    "    plt.legend([\"l1 penalty\", \"l2 penalty\"])\n",
    "    plt.xlabel(\"C parameter\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xscale(\"log\", basex=10)\n",
    "    plt.title(\"Results from logistic regression classifier\")\n",
    "    plt.show()\n",
    "    return (best_classifier[0], {\"penalty\": best_classifier[2], \"c\": best_classifier[1]}, best_classifier[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 8.33 s, total: 1min 38s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_x = np.loadtxt(\"./Datasets/train_x.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 118 ms, sys: 1.91 ms, total: 120 ms\n",
      "Wall time: 146 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_y = np.loadtxt(\"./Datasets/train_y.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 s, sys: 680 ms, total: 17.8 s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_x = np.loadtxt(\"./Datasets/test_x.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_train_x = np.loadtxt(\"./Datasets/mini_train_x.csv\", delimiter=',')\n",
    "mini_train_y = np.loadtxt(\"./Datasets/mini_train_y.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# train_avg = np.mean(train_x, axis=0)\n",
    "# for i in range(len(train_x)):\n",
    "#     train_x[i] -= train_avg\n",
    "# for i in range(len(test_x)):\n",
    "#     test_x[i] -= train_avg\n",
    "# train_x /= 255.0\n",
    "# test_x /= 255.0\n",
    "\n",
    "# X_train, y_train = unison_shuffled_copies(train_x, train_y)\n",
    "\n",
    "# HYP_TUNE_SIZE = 5000\n",
    "# train = (X_train[:int(HYP_TUNE_SIZE * 0.7)], y_train[:int(HYP_TUNE_SIZE*0.7)])\n",
    "# valid = (X_train[int(HYP_TUNE_SIZE*0.7):], y_train[int(HYP_TUNE_SIZE*0.7):])\n",
    "# results_logistic = logistic_regression(train, valid, np.logspace(-6,6,20), verbose=True)\n",
    "# results_svm = linear_svm(train, valid, np.logspace(-6,6,20), verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "from __future__ import print_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into tensors\n",
    "x_train, y_train = unison_shuffled_copies(train_x, train_y)\n",
    "\n",
    "x_train_dev, x_test_dev, y_train_dev, y_test_dev = train_test_split(x_train, y_train, test_size=0.05, train_size=0.1, random_state=42)\n",
    "\n",
    "ttrain_x = torch.from_numpy(x_train_dev) # currently has x_train_dev as dataset for dev purposes\n",
    "ttrain_y = torch.from_numpy(y_train_dev)\n",
    "ttest_x = torch.from_numpy(x_test_dev)\n",
    "ttest_y = torch.from_numpy(y_test_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Can choose to take dimensions in constructor, but for now just keeping them variable since unsure about\n",
    "    # how many variables to have in the constructor based on design\n",
    "    def __init__(self, scale):\n",
    "        self.print = True\n",
    "        self.scale = scale\n",
    "        super(CNN, self).__init__() # init recursively\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(p=0.1), # Random p% of nodes are cancelled - this is for regularization\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(p=0.1), # Random p% of nodes are cancelled - this is for regularization\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "#         self.layer4 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "#         )\n",
    "         # Logistic Regression\n",
    "        self.fc1 = nn.Linear(self.scale*64*64,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "        out = out.view( -1 , self.scale*64*batch_size)\n",
    "        out = self.fc1(out)\n",
    "        if(self.print):\n",
    "            self.print = False\n",
    "            print(\"output size: \", out.size())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "class DatasetKaggle(data.Dataset):\n",
    "    def __init__(self, np_data_x, np_data_y, transform=None):\n",
    "        self.data = np_data_x\n",
    "        self.labels = np_data_y\n",
    "        if (transform is not None):\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = transforms.ToTensor()\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        sample = self.data[index]\n",
    "        sample = sample.reshape(64,64,1)\n",
    "        sample = self.transform(sample)\n",
    "        \n",
    "        \n",
    "        label = int(self.labels[index])\n",
    "        \n",
    "        return sample, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = []\n",
    "# for i in range(len(x_train_dev)):\n",
    "#     train.append((x_train_dev[i],y_train_dev[i]))\n",
    "\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "# train = torch.utils.data.TensorDataset(ttrain_x,ttrain_y)\n",
    "\n",
    "\n",
    "\n",
    "train = DatasetKaggle(x_train_dev, y_train_dev)\n",
    "test = DatasetKaggle(x_test_dev, y_test_dev)\n",
    "\n",
    "# train_x = []\n",
    "# train_y = []\n",
    "# for i in range(len(y_train_dev)):\n",
    "#     train_x.append(x_train_dev[i].reshape(64,64,1))\n",
    "#     train_y.append(int(y_train_dev[i]))\n",
    "\n",
    "# transform = transforms.ToTensor()\n",
    "# tensor_x = torch.stack([transform(i) for i in train_x]) # transform to torch tensors\n",
    "# tensor_y = torch.LongTensor(train_y)\n",
    "\n",
    "# ttrain = torch.utils.data.TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "mnist_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  torch.Size([64, 10])\n",
      "Epoch : 0 Loss : 2.524 \n",
      "Epoch : 0 Test Acc : 12.160\n",
      "Time for epoch:  302.8651117779955\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-16:\n",
      "Process Process-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7fc35258b470>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 331, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/usr/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 23077) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f22073b56c74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Reset gradients to zero, perform a backward pass, and update the weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = CNN(32)\n",
    "if cuda_available:\n",
    "    clf = clf.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# y_onehot = torch.LongTensor(batch_size, 10)\n",
    "for epoch in range(50):\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "    losses = []\n",
    "    # Train\n",
    "    for batch_idx, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "#         y = torch.LongTensor(batch_size,1)\n",
    "#         for i in range(batch_size):\n",
    "#             y[i] = labels[i]\n",
    "#         y_onehot.zero_()\n",
    "#         y_onehot.scatter_(1, y, 1)\n",
    "#         labels = y_onehot\n",
    "        \n",
    "        if cuda_available:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "    \n",
    "        outputs = clf(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(outputs,labels)\n",
    "        \n",
    "        # Reset gradients to zero, perform a backward pass, and update the weights.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data[0])\n",
    "\n",
    "    print('Epoch : %d Loss : %.3f ' % (epoch, np.mean(losses)))\n",
    "    \n",
    "    # Evaluate\n",
    "    clf.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "        if cuda_available:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "        outputs = clf(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print('Epoch : %d Test Acc : %.3f' % (epoch, 100.*correct/total))\n",
    "    print('Time for epoch: ', elapsed)\n",
    "    print('--------------------------------------------------------------')\n",
    "    clf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
