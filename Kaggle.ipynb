{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shuffle two arrays, keeping rows in correspondence\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def linear_svm(train, valid, c_vals, verbose=False):\n",
    "    train_x = train[0]\n",
    "    train_y = train[1]\n",
    "    valid_x = valid[0]\n",
    "    valid_y = valid[1]\n",
    "    \n",
    "    best_classifier = (None, 0, \"l1\", 0)\n",
    "\n",
    "    res = {\"l1\": [], \"l2\": []}\n",
    "    \n",
    "    for penalty in [\"l2\"]:\n",
    "        for c in c_vals:\n",
    "            clf = LinearSVC(C=c, penalty=penalty)\n",
    "            clf.fit(train_x, train_y)\n",
    "            prediction = clf.predict(valid_x)\n",
    "            accuracy = accuracy_score(valid_y, prediction)\n",
    "            res[penalty].append(accuracy)\n",
    "            if (accuracy > best_classifier[3]):\n",
    "                best_classifier = (clf, c, penalty, accuracy)\n",
    "            if verbose:\n",
    "                print(\"Tried c = \" + str(c) + \" with \" + penalty + \" penalty\")\n",
    "    #plt.plot(c_vals, res[\"l1\"])\n",
    "    plt.plot(c_vals, res[\"l2\"])\n",
    "    #plt.legend([\"l1 penalty\", \"l2 penalty\"])\n",
    "    plt.xlabel(\"C parameter\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xscale(\"log\", basex=10)\n",
    "    plt.title(\"Results from linear SVM classifier\")\n",
    "    plt.show()\n",
    "    return (best_classifier[0], {\"penalty\": best_classifier[2], \"c\": best_classifier[1]}, best_classifier[3])\n",
    "\n",
    "# The training and test sets are passed as tuples where the first index is the X and the second is the Y\n",
    "def logistic_regression(train, valid, c_vals, verbose=False):\n",
    "    train_x = train[0]\n",
    "    train_y = train[1]\n",
    "    valid_x = valid[0]\n",
    "    valid_y = valid[1]\n",
    "    \n",
    "    best_classifier = (None, 0, \"l1\", 0)\n",
    "\n",
    "    res = {\"l1\": [], \"l2\": []}\n",
    "    \n",
    "    for penalty in [\"l1\", \"l2\"]:\n",
    "        for c in c_vals:\n",
    "            clf = LogisticRegression(C=c, penalty=penalty)\n",
    "            clf.fit(train_x, train_y)\n",
    "            prediction = clf.predict(valid_x)\n",
    "            accuracy = accuracy_score(valid_y, prediction)\n",
    "            res[penalty].append(accuracy)\n",
    "            if (accuracy > best_classifier[3]):\n",
    "                best_classifier = (clf, c, penalty, accuracy)\n",
    "            if verbose:\n",
    "                print(\"Tried c = \" + str(c) + \" with \" + penalty + \" penalty\")\n",
    "    plt.plot(c_vals, res[\"l1\"])\n",
    "    plt.plot(c_vals, res[\"l2\"])\n",
    "    plt.legend([\"l1 penalty\", \"l2 penalty\"])\n",
    "    plt.xlabel(\"C parameter\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xscale(\"log\", basex=10)\n",
    "    plt.title(\"Results from logistic regression classifier\")\n",
    "    plt.show()\n",
    "    return (best_classifier[0], {\"penalty\": best_classifier[2], \"c\": best_classifier[1]}, best_classifier[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 35.7 s, total: 2min 12s\n",
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_x = np.loadtxt(\"./Datasets/train_x.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 144 ms, sys: 7.97 ms, total: 152 ms\n",
      "Wall time: 181 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_y = np.loadtxt(\"./Datasets/train_y.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 s, sys: 649 ms, total: 18 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_x = np.loadtxt(\"./Datasets/test_x.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_train_x = np.loadtxt(\"./Datasets/mini_train_x.csv\", delimiter=',')\n",
    "mini_train_y = np.loadtxt(\"./Datasets/mini_train_y.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# train_avg = np.mean(train_x, axis=0)\n",
    "# for i in range(len(train_x)):\n",
    "#     train_x[i] -= train_avg\n",
    "# for i in range(len(test_x)):\n",
    "#     test_x[i] -= train_avg\n",
    "# train_x /= 255.0\n",
    "# test_x /= 255.0\n",
    "\n",
    "# X_train, y_train = unison_shuffled_copies(train_x, train_y)\n",
    "\n",
    "# HYP_TUNE_SIZE = 5000\n",
    "# train = (X_train[:int(HYP_TUNE_SIZE * 0.7)], y_train[:int(HYP_TUNE_SIZE*0.7)])\n",
    "# valid = (X_train[int(HYP_TUNE_SIZE*0.7):], y_train[int(HYP_TUNE_SIZE*0.7):])\n",
    "# results_logistic = logistic_regression(train, valid, np.logspace(-6,6,20), verbose=True)\n",
    "# results_svm = linear_svm(train, valid, np.logspace(-6,6,20), verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "from __future__ import print_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into tensors\n",
    "x_train, y_train = unison_shuffled_copies(train_x, train_y)\n",
    "\n",
    "x_train_dev, x_test_dev, y_train_dev, y_test_dev = train_test_split(x_train, y_train, test_size=0.1, train_size=0.3, random_state=42)\n",
    "\n",
    "ttrain_x = torch.from_numpy(x_train_dev) # currently has x_train_dev as dataset for dev purposes\n",
    "ttrain_y = torch.from_numpy(y_train_dev)\n",
    "ttest_x = torch.from_numpy(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Can choose to take dimensions in constructor, but for now just keeping them variable since unsure about\n",
    "    # how many variables to have in the constructor based on design\n",
    "    def __init__(self, batch_size):\n",
    "        super(CNN, self).__init__() # init recursively\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(p=0.1), # Random p% of nodes are cancelled - this is for regularization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        )\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "#         )\n",
    "#         self.layer3 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.Dropout(p=0.1), # Random p% of nodes are cancelled - this is for regularization\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         self.layer4 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "#         )\n",
    "         # Logistic Regression\n",
    "        self.fc1 = nn.Linear(64*64*batch_size,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "        out = out.view( -1 , 64*64*batch_size)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# class CNN(nn.Module):\n",
    "#     \"\"\"Convnet Classifier\"\"\"\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             # Layer 1\n",
    "#             nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "#             # Layer 2\n",
    "#             nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "#             # Layer 3\n",
    "#             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "#             # Layer 4\n",
    "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "#         )\n",
    "#         # Logistic Regression\n",
    "#         self.clf = nn.Linear(128, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.clf(self.conv(x).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "class DatasetKaggle(data.Dataset):\n",
    "    def __init__(self, np_data_x, np_data_y, transform=None):\n",
    "        self.data = np_data_x\n",
    "        self.labels = np_data_y\n",
    "        if (transform is not None):\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = transforms.ToTensor()\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        sample = self.data[index]\n",
    "        sample = sample.reshape(64,64,1)\n",
    "        sample = self.transform(sample)\n",
    "        print(sample.size())\n",
    "        \n",
    "        \n",
    "        label = int(self.labels[index])\n",
    "        \n",
    "        return sample, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = []\n",
    "# for i in range(len(x_train_dev)):\n",
    "#     train.append((x_train_dev[i],y_train_dev[i]))\n",
    "\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "# train = torch.utils.data.TensorDataset(ttrain_x,ttrain_y)\n",
    "\n",
    "\n",
    "\n",
    "# train = DatasetKaggle(x_train_dev, y_train_dev)\n",
    "\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "for i in range(len(y_train_dev)):\n",
    "    train_x.append(x_train_dev[i].reshape(64,64,1))\n",
    "    train_y.append(int(y_train_dev[i]))\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "tensor_x = torch.stack([transform(i) for i in train_x]) # transform to torch tensors\n",
    "tensor_y = torch.LongTensor(train_y)\n",
    "\n",
    "ttrain = torch.utils.data.TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "mnist_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(ttrain, batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "# testloader = torch.utils.data.DataLoader(ttrain_y, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0740\n",
      "-1.1715\n",
      " 0.1913\n",
      " 0.1722\n",
      " 0.8877\n",
      "-0.7376\n",
      " 0.4114\n",
      "-0.4804\n",
      " 0.3377\n",
      " 0.4205\n",
      "[torch.FloatTensor of size 10]\n",
      "\n",
      "Variable containing:\n",
      " 8\n",
      "[torch.LongTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `THIndexTensor_(size)(target, 0) == batch_size' failed.  at /build/python-pytorch/src/pytorch-0.3.1-py3/torch/lib/THNN/generic/ClassNLLCriterion.c:79",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-0031d686a97e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Reset gradients to zero, perform a backward pass, and update the weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \"\"\"\n\u001b[0;32m-> 1161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `THIndexTensor_(size)(target, 0) == batch_size' failed.  at /build/python-pytorch/src/pytorch-0.3.1-py3/torch/lib/THNN/generic/ClassNLLCriterion.c:79"
     ]
    }
   ],
   "source": [
    "clf = CNN(batch_size)\n",
    "if cuda_available:\n",
    "    clf = clf.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# y_onehot = torch.LongTensor(batch_size, 10)\n",
    "for epoch in range(50):\n",
    "    losses = []\n",
    "    # Train\n",
    "    for batch_idx, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "#         y = torch.LongTensor(batch_size,1)\n",
    "#         for i in range(batch_size):\n",
    "#             y[i] = labels[i]\n",
    "#         y_onehot.zero_()\n",
    "#         y_onehot.scatter_(1, y, 1)\n",
    "#         labels = y_onehot\n",
    "        \n",
    "        if cuda_available:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "    \n",
    "        outputs = clf(inputs)\n",
    "        print(outputs[0])\n",
    "        print(labels[0])\n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        \n",
    "        # Reset gradients to zero, perform a backward pass, and update the weights.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data[0])\n",
    "\n",
    "    print('Epoch : %d Loss : %.3f ' % (epoch, np.mean(losses)))\n",
    "    \n",
    "    # Evaluate\n",
    "    clf.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "        if cuda_available:\n",
    "            inputs, labels = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "        outputs = clf(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    print('Epoch : %d Test Acc : %.3f' % (epoch, 100.*correct/total))\n",
    "    print('--------------------------------------------------------------')\n",
    "    clf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
